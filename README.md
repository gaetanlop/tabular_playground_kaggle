# Tabular_playground_kaggle (February)

## Complete roadmap on how I worked on this competition

* 11/02: Hyperparameter tuning using Optuna
* 12/02: Decision Tree/ XGBoost hyperparameter tuning
* 13/02: Practical implementation of XGBoost and Optuna
* 15/02: Shap Values Theory + practical Implementation
* 16/02: Tried some Feature Engineering ideas with no success + 10 folds did not improve the model
* 17/02: lgbm theory + implementation using optuna
* 18/02: XGBoost Optuna cross val didnot work + cat boost implementation
* 19/02: theory + lgbm optimization
* 20/02: lgbm theory + implementation of how to hyper tune lgbm
* 21/02: lgbm classification
* 22/02: tried to implement a model that classifies the target between two classes then train a model but was doing worse
* 23/02: Find the best hyperparameters for extreme tuning using Optuna + xgboost theory
* 24/02: submitting the best hyperparameters for extreme tuning + ensemble with a good notebook from comparative method
* 25/02 : tried to find a way to improve performance of my best model using extreme fine tuning
* 26/02 : Improve extreme fine tuning
* 28/02 : Find an explanation about extreme fine tuning


# Tabular_playground_kaggle (March)

* 01/03/2021 : Exploratory Data Analysis + Optuna hyperparameter tuning
* 02/03/2021 : Predictions using Stratified KFold + Extreme Parameter tuning strategy

Working on: - 6 hours TPS for Extreme hyperparameter tuning
- TPS3 hyperparameter tuning to find the best hyperparameters ...
